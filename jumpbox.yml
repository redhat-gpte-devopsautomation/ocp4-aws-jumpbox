---
- name: BEGIN setup jumpbox playbook
  hosts: localhost
  connection: local
  vars:
    HOME: "{{ lookup('env', 'HOME') }}"
    # need to set GUID because this playbook not run under agnosticD
    GUID: "{{ lookup('env', 'GUID') }}"
    ssh_config: "{{ lookup('env', 'HOME') }}/.ssh/config"
    cluster_key_filename: "{{ lookup('env', 'HOME') }}/.ssh/id_rsa"

  tasks:

    - name: find proper ssh keyfile - stat the cluster-GUID-key version like in GPTE classes
      stat:
        path: "{{ lookup('env', 'HOME') }}/.ssh/cluster-{{ GUID }}-key"
      register: r_cluster_key_stat

    - name: find proper ssh keyfile - set the cluster-GUID-key version like in GPTE classes
      set_fact:
        cluster_key_filename: "{{ lookup('env', 'HOME') }}/.ssh/cluster-{{ GUID }}-key"
      when: r_cluster_key_stat.stat.exists

    - name: set GUID as fact
      set_fact:
        GUID: "{{ lookup('env', 'GUID') }}"
    
    - name: find the clientvm's region
      uri:
        url: http://169.254.169.254/latest/meta-data/placement/availability-zone
        return_content: true
      register: r_aws_region

    - name: set fact of aws_region
      set_fact:
        aws_region: "{{ r_aws_region.content|regex_replace('.$', '') }}"

    - name: add region to aws client config
      ini_file:
        path: "{{ HOME }}/.aws/credentials"
        section: "{{ item }}"
        option: region
        value: "{{ aws_region }}"
      loop:
        - default
        - opentlc-labs

    - name: get latest AMI id for RHEL in my region
      shell: |
        aws ec2 describe-images \
          --owners 309956199498 \
          --query 'sort_by(Images, &CreationDate)[-1].[ImageId]' \
          --filters "Name=name,Values=RHEL-8*" \
          --region {{ aws_region }} \
          --output text
      register: r_rhel_ami_id

    - name: setup RHEL ami id
      set_fact:
        rhel_ami_id: "{{ r_rhel_ami_id.stdout }}"

    - name: clientvm setup
      include_tasks:
        file: ./setup_clientvm.yml

    - name: get control plane nodes
      ec2_instance_facts:
        profile: default
      register: all_instances

    - name: get control plane nodes
      ec2_instance_facts:
        profile: default
        filters:
          "tag:Name": "*master*"
      register: control_plane

    - name: add control_planes to inventory
      add_host:
        name: "{{ item.private_dns_name }}"
        groups: control_plane
      loop: "{{ control_plane.instances }}"
      when: control_plane.instances[0] is defined

    - name: get workers
      ec2_instance_facts:
        profile: default
        filters:
          "tag:Name": "*worker*"
      register: workers

    - name: add workers to inventory
      add_host:
        name: "{{ item.private_dns_name }}"
        groups: workers
      loop: "{{ workers.instances }}"
      when: workers.instances[0] is defined

    - name: is there a jumpbox?
      ec2_instance_facts:
        profile: default
        filters:
          "tag:type": "jumpbox"
          instance-state-name: ["running", "initializing"]
      register: jumpboxes



    #  - name: add existing jumpbox to inventory
    #    add_host:
    #      name: "{{ jumpboxes.instances[0].public_ip_address }}"
    #      groups: jumpbox
    #    when: jumpboxes.instances[0] is defined

    # if there's no jumpbox already..
    - when: jumpboxes["instances"][0] is not defined
      block:

        - name: upload the cluster keypair to AWS for jumphost
          ec2_key:
            profile: default
            name: cluster_keypair
            key_material: "{{ lookup('file', cluster_key_filename + '.pub') }}"

        - name: Create SSH security group
          vars:
            cluster_vpc: "{{ control_plane.instances[0].vpc_id }}"
            #cluster_region: "{{ control_plane.instances[0].placement.availability_zone| regex_replace('.$') }}"
          ec2_group:
            profile: default
            name: jumpbox
            description: jumpbox description
            region: "{{ aws_region }}"
            vpc_id: "{{ cluster_vpc }}"
            rules:
              - proto: tcp
                ports: 22
                cidr_ip: 0.0.0.0/0
          register: sg_jumpbox

        - name: get vpc_subnet_id
          ec2_vpc_subnet_facts:
            profile: default
            filters:
              vpc-id: "{{ sg_jumpbox.vpc_id }}"
              cidr-block: "10.0.0.0/20"
          register: ec2_vpc_subnet_ids

        - name: launch ec2 instance
          ec2:
            # name: "jumpbox"
            profile: default
            key_name: "cluster_keypair"
            vpc_subnet_id: "{{ ec2_vpc_subnet_ids.subnets[0].subnet_id }}"
            instance_type: t2.micro
            group_id: "{{ sg_jumpbox.group_id }}"
            # security_group: "{{ sg_jumpbox.group_id }}"
            # security_group: "jumpbox"
            # image_id: ami-0520e698dd500b1d1
            # image: ami-0520e698dd500b1d1
            image: "{{ rhel_ami_id }}"
            region: "{{ aws_region }}"
            assign_public_ip: true
            instance_tags:
              type: jumpbox
            wait: true
          register: jumpbox

        - name: Add jumpbox instance public IP to host group
          add_host:
            name: "{{ jumpbox.instances[0].public_ip }}"
            groups: jumpbox

        - name: delete clientvm ssh config
          file:
            dest: "{{ ssh_config }}"
            state: absent

        - name: create empty ssh config
          file:
            dest: "{{ ssh_config }}"
            state: touch
            mode: 0600

        - name: setup clientvm ssh config
          blockinfile:
            dest: "{{ ssh_config }}"
            marker: "##### {mark} Adding masters with ProxyJump"
            content: |
              Host {{ jumpbox.instances[0].public_ip }}
                User ec2-user
                StrictHostKeyChecking no

              Host *.internal
                User core
                ProxyJump {{ jumpbox.instances[0].public_ip }}
                StrictHostKeyChecking no

              Match User ec2-user
                IdentityFile {{ cluster_key_filename }}
                StrictHostKeyChecking no

              Match User core
                IdentityFile {{ cluster_key_filename }}
                StrictHostKeyChecking no

              Host *
                ControlMaster auto
                ControlPath /tmp/%h-%r
                ControlPersist 5m
                StrictHostKeyChecking no

        - name: Wait for SSH to come up
          delegate_to: "{{ jumpbox.instances[0].public_ip }}"
          wait_for_connection:
            delay: 10
            timeout: 180

    - name: add existing jumpbox to inventory
      add_host:
        name: "{{ jumpboxes.instances[0].public_ip_address }}"
        groups: jumpbox
      when: jumpboxes.instances[0] is defined

- name: prep jumpbox
  hosts: jumpbox
  vars:
    HOME: "{{ lookup('env', 'HOME') }}"
    ssh_config: "{{ ansible_env.HOME }}/.ssh/config"

  tasks:

    - name: put kubeconfig on jumpbox
      copy:
        src: "{{ HOME }}/cluster-{{ hostvars.localhost.GUID }}/auth/kubeconfig"
        dest: "{{ ansible_env.HOME }}/kubeconfig"

    - debug: var=hostvars.localhost


    - name: copy cluster ssh key to jumpbox
      copy:
        src: "{{ hostvars.localhost.real_cluster_key_filename }}"
        dest: "{{ ansible_env.HOME }}/.ssh/id_rsa"
        mode: 0600


    - name: delete jumpbox ssh config
      file:
        dest: "{{ ssh_config }}"
        state: absent

    - name: create empty ssh config
      file:
        dest: "{{ ssh_config }}"
        state: touch
        mode: 0600

    - name: setup ssh config on jumpbox
      blockinfile:
        dest: "{{ ssh_config }}"
        marker: "##### {mark} ADD default key and user"
        content: |
          Host *
            IdentityFile ~/.ssh/id_rsa
            User core
            StrictHostKeyChecking no
            ControlPath /tmp/{{ hostvars.localhost.GUID }}-%r-%h-%p
            ControlPersist 5m

    - name: show jumpbox info
      debug:
        # var: groups["jumpbox"].host_name
        msg:
         - "SSH to Jumpbox: ssh {{ groups['jumpbox'][0] }}"
         - "SSH to Master: ssh {{ groups['control_plane'][0] }}"


